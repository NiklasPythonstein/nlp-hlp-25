{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze-controlled text generation\n",
    "\n",
    "This notebook demonstrates how to generate texts with the language model / gaze model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46266a09cbf4e9e968b226abc1b45ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to huggingface to run models. Requires the environment variable \"HUGGINGFACE_TOKEN\" to be set with a valid access token.\n",
    "# Linux: export HUGGINGFACE_TOKEN=your_huggingface_token_here\n",
    "# Windows: $env:HUGGINGFACE_TOKEN=\"your_huggingface_token_here\"\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from modeling.gaze_models import CausalTransformerGazeModel\n",
    "from modeling.generation import GazeControlledBeamSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "The texts in our experiment were generated using the off-the-shelf instruction-tuned Llama-3.2 language model with 3B parameters. The gaze model is a GPT-2 model fine-tuned to predict first-pass gaze duration.\n",
    "\n",
    "> **NOTE:** Expect the ensemble to be quite slow on CPU (up to a minute per token), so you should consider either using a GPU (e.g., on Google Colab) or choosing smaller model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "language_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "gaze_model_name = \"openai-community/gpt2\"\n",
    "\n",
    "language_model_tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "language_model = AutoModelForCausalLM.from_pretrained(language_model_name).to(device)\n",
    "\n",
    "gaze_model = CausalTransformerGazeModel.from_pretrained(gaze_model_name).to(device)\n",
    "gaze_model.load_state_dict(torch.load(\"models/trf_gaze_model.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the story prompts\n",
    "\n",
    "The titles and prompts for the stories were generated using GPT-4 and manually curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stories/prompts.jsonl\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "pprint(prompts, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the texts\n",
    "\n",
    "Before starting the generation, we need to build an instruction prompt according to the template that is specific to Llama-3.2. We then generate text using [beam search](https://en.wikipedia.org/wiki/Beam_search) until one of two conditions applies:\n",
    "\n",
    "- the language model has predicted an end-of-message token in the best beam, or\n",
    "- the number of generated tokens has reached 800.\n",
    "\n",
    "![Visualization of beam search with beam size 3](https://upload.wikimedia.org/wikipedia/commons/2/23/Beam_search.gif)\n",
    "\n",
    "Refer to [`generation.py`](modeling/generation.py) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_weight = 2\n",
    "beam_size = 8\n",
    "\n",
    "beam_search = GazeControlledBeamSearch(\n",
    "    language_model,\n",
    "    language_model_tokenizer,\n",
    "    gaze_model,\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "for prompt in prompts:\n",
    "    input_text = language_model_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Write a short story based on the following title and prompt.\\n\"\n",
    "                    f\"Title: {prompt['title']}\\n\"\n",
    "                    f\"Prompt: {prompt['prompt']}\\n\\n\"\n",
    "                    \"The story should not be longer than 500 words. \"\n",
    "                    \"Keep in mind that the reader will not see the prompt, only the story itself. \"\n",
    "                    \"Do not include the title.\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    output_text, token_score, gaze_score = beam_search.generate(\n",
    "        input_text,\n",
    "        gaze_weight=gaze_weight,\n",
    "        max_length=800,\n",
    "        beam_size=beam_size,\n",
    "        ignore_prompt=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    outputs.append(\n",
    "        {\n",
    "            **prompt,\n",
    "            \"gaze_weight\": gaze_weight,\n",
    "            \"input_text\": input_text,\n",
    "            \"output_text\": output_text,\n",
    "            \"token_score\": token_score,\n",
    "            \"gaze_score\": gaze_score,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the outputs\n",
    "\n",
    "This includes the final text from the best beam as well as the total token score from the language model and the gaze score from the gaze model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stories/outputs.jsonl\", \"w\") as f:\n",
    "    for output in outputs:\n",
    "        f.write(json.dumps(output) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
